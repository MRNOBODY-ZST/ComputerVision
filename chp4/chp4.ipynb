{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c1cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7846fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义LeNet-5网络结构\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # 第一层卷积: 1个输入通道, 6个输出通道, 5x5卷积核\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)\n",
    "        # 第二层卷积: 6个输入通道, 16个输出通道, 5x5卷积核\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        # 激活函数和池化层\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 第一层: 卷积 -> ReLU -> 池化\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        # 第二层: 卷积 -> ReLU -> 池化\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        # 展平\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        # 全连接层\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "074934bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 统计\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    print(f'Train set: Average loss: {avg_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.2f}%)')\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375c767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            \n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{total} ({accuracy:.2f}%)\\n')\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练结果\n",
    "def plot_results(train_losses, train_accs, test_losses, test_accs):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # 损失曲线\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n",
    "    plt.plot(epochs, test_losses, 'r-', label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Test Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 准确率曲线\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accs, 'b-', label='Train Accuracy')\n",
    "    plt.plot(epochs, test_accs, 'r-', label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/training_results.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"训练结果图已保存到 training_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e25f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示一些预测结果\n",
    "def visualize_predictions(model, device, test_loader, num_images=10):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images, labels = images[:num_images].to(device), labels[:num_images]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predictions = outputs.max(1)\n",
    "    \n",
    "    images = images.cpu()\n",
    "    predictions = predictions.cpu()\n",
    "    \n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "        plt.title(f'True: {labels[i]}\\nPred: {predictions[i]}',\n",
    "                  color='green' if labels[i] == predictions[i] else 'red')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/predictions.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"预测结果图已保存到 predictions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697c9a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "正在下载和加载MNIST数据集...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [03:05<00:00, 53.5kB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 138kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:20<00:00, 79.0kB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 14.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 60000\n",
      "测试集大小: 10000\n",
      "\n",
      "LeNet-5 网络结构:\n",
      "LeNet5(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "\n",
      "总参数量: 61,706\n",
      "\n",
      "开始训练...\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304433\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.446174\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.244137\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.136241\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.162051\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.206196\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.077167\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.342371\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.076813\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.048798\n",
      "Train set: Average loss: 0.2203, Accuracy: 55898/60000 (93.16%)\n",
      "Test set: Average loss: 0.0726, Accuracy: 9748/10000 (97.48%)\n",
      "\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.116287\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.068865\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.023925\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.060644\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.029039\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.093194\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.135562\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.023569\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.197768\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.090311\n",
      "Train set: Average loss: 0.0657, Accuracy: 58780/60000 (97.97%)\n",
      "Test set: Average loss: 0.0455, Accuracy: 9855/10000 (98.55%)\n",
      "\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.067579\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.006511\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.013992\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003137\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.008487\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.077164\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.002197\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.066335\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.036340\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.040075\n",
      "Train set: Average loss: 0.0476, Accuracy: 59111/60000 (98.52%)\n",
      "Test set: Average loss: 0.0363, Accuracy: 9879/10000 (98.79%)\n",
      "\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.104552\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.012578\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.068478\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.018287\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.019922\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.003988\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.109740\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.023399\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.062864\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.001364\n",
      "Train set: Average loss: 0.0384, Accuracy: 59263/60000 (98.77%)\n",
      "Test set: Average loss: 0.0356, Accuracy: 9876/10000 (98.76%)\n",
      "\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.004971\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.059572\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.001737\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.007691\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.021949\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.007617\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.019016\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.015702\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.070484\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.015314\n",
      "Train set: Average loss: 0.0315, Accuracy: 59398/60000 (99.00%)\n",
      "Test set: Average loss: 0.0336, Accuracy: 9896/10000 (98.96%)\n",
      "\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.008460\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.001262\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.014564\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.001598\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.039565\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.015657\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.015452\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.048749\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.013402\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.029598\n",
      "Train set: Average loss: 0.0266, Accuracy: 59467/60000 (99.11%)\n",
      "Test set: Average loss: 0.0355, Accuracy: 9890/10000 (98.90%)\n",
      "\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.019668\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.011669\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.003770\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.006192\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.006918\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.014230\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.003719\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.015945\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.001268\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001374\n",
      "Train set: Average loss: 0.0223, Accuracy: 59583/60000 (99.31%)\n",
      "Test set: Average loss: 0.0376, Accuracy: 9881/10000 (98.81%)\n",
      "\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.028459\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.007941\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.009135\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.001785\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.002288\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.000702\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.011559\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.075369\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.086597\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.005633\n",
      "Train set: Average loss: 0.0185, Accuracy: 59623/60000 (99.37%)\n",
      "Test set: Average loss: 0.0434, Accuracy: 9875/10000 (98.75%)\n",
      "\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.072204\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.006327\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.005785\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000427\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.006254\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.002921\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.001223\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002205\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.000326\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.018656\n",
      "Train set: Average loss: 0.0157, Accuracy: 59695/60000 (99.49%)\n",
      "Test set: Average loss: 0.0446, Accuracy: 9871/10000 (98.71%)\n",
      "\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001880\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.007704\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.017736\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.002529\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.014940\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.006089\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.046873\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.000361\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.006822\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.165886\n",
      "Train set: Average loss: 0.0146, Accuracy: 59715/60000 (99.53%)\n",
      "Test set: Average loss: 0.0386, Accuracy: 9891/10000 (98.91%)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory /mnt/user-data/outputs does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m最终测试准确率: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accs[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     57\u001b[39m     test_accs.append(test_acc)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# 保存模型\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/mnt/user-data/outputs/lenet5_mnist.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m模型已保存到 lenet5_mnist.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# 可视化结果\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\28567\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:943\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    940\u001b[39m _check_save_filelike(f)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    944\u001b[39m         _save(\n\u001b[32m    945\u001b[39m             obj,\n\u001b[32m    946\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    949\u001b[39m             _disable_byteorder_record,\n\u001b[32m    950\u001b[39m         )\n\u001b[32m    951\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\28567\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:810\u001b[39m, in \u001b[36m_open_zipfile_writer\u001b[39m\u001b[34m(name_or_buffer)\u001b[39m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    809\u001b[39m     container = _open_zipfile_writer_buffer\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\28567\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:781\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__init__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    777\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    778\u001b[39m         torch._C.PyTorchFileWriter(\u001b[38;5;28mself\u001b[39m.file_stream, _compute_crc32)\n\u001b[32m    779\u001b[39m     )\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_compute_crc32\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Parent directory /mnt/user-data/outputs does not exist."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 超参数设置\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 10\n",
    "    \n",
    "    # 设置设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'使用设备: {device}')\n",
    "    \n",
    "    # 数据预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # MNIST数据集的均值和标准差\n",
    "    ])\n",
    "    \n",
    "    # 加载数据集\n",
    "    print(\"正在下载和加载MNIST数据集...\")\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"训练集大小: {len(train_dataset)}\")\n",
    "    print(f\"测试集大小: {len(test_dataset)}\")\n",
    "    \n",
    "    # 创建模型\n",
    "    model = LeNet5().to(device)\n",
    "    print(\"\\nLeNet-5 网络结构:\")\n",
    "    print(model)\n",
    "    \n",
    "    # 计算模型参数量\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\n总参数量: {total_params:,}\")\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 训练模型\n",
    "    train_losses, train_accs = [], []\n",
    "    test_losses, test_accs = [], []\n",
    "    \n",
    "    print(\"\\n开始训练...\")\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        train_loss, train_acc = train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "        test_loss, test_acc = test(model, device, test_loader, criterion)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), '/mnt/user-data/outputs/lenet5_mnist.pth')\n",
    "    print(\"模型已保存到 lenet5_mnist.pth\")\n",
    "    \n",
    "    # 可视化结果\n",
    "    plot_results(train_losses, train_accs, test_losses, test_accs)\n",
    "    visualize_predictions(model, device, test_loader)\n",
    "    \n",
    "    print(\"\\n训练完成!\")\n",
    "    print(f\"最终测试准确率: {test_accs[-1]:.2f}%\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
